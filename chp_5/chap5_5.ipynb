{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chap5-5.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT3BhfK4wW1Q"
      },
      "source": [
        "!git clone https://github.com/1stDayHack/FDK.git\r\n",
        "!pip install -r FDK/requirements.txt\r\n",
        "!pip install matplotlib==3.1.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq7DnvBAwXdX"
      },
      "source": [
        "import torch\r\n",
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2fAvA1mwac9"
      },
      "source": [
        "!python -m pip install detectron2 -f \\\r\n",
        "  https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WUwFBgLwctF"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf91MJx0wfbK"
      },
      "source": [
        "%cp drive/MyDrive/A_zz_random_stuffs/Books/1stdayhackReadings/RRDB_ESRGAN_x4.pth FDK/src/core/base_libs/ESRGAN/models/RRDB_ESRGAN_x4.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5730uAGRw5Wf"
      },
      "source": [
        "#Import libs\r\n",
        "from FDK.src.core.detect import Detector\r\n",
        "from FDK.src.core.super_res import SuperReser\r\n",
        "from FDK.src.core.utils import utils\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e3c3B0pw9NW"
      },
      "source": [
        "det = Detector(model=\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSLbkecAxQo4"
      },
      "source": [
        "from pprint import pprint\r\n",
        "pprint(det.cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbLYrxVgxZBZ"
      },
      "source": [
        "img = Image.open(\"kisedje.jpeg\")\r\n",
        "img_cv = utils.pil_to_cv2(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOS0Sc_vzLSE"
      },
      "source": [
        "output = det.predict(img_cv)\r\n",
        "out_img = det.visualize(img_cv, output, figsize=(18,18))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "S2k2P_PRzqiT"
      },
      "source": [
        "pprint(det.dataset_metadata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okF9oke14LMQ"
      },
      "source": [
        "# 5.5 Tutorial Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYmhcWTB4r2D"
      },
      "source": [
        "1. Use `Detector` from the 1stDayKit or FDK module of `detect` to perform the task of object-detection, object-segmentation and object-classification on a test image. For example, you can try it out on this: [`https://mymodernmet.com/wp/wp-content/uploads/2018/01/Tatsuto-Shibata-tokyo-photography-13.jpg`](https://mymodernmet.com/wp/wp-content/uploads/2018/01/Tatsuto-Shibata-tokyo-photography-13.jpg) \r\n",
        "\r\n",
        "***Hint***: If you are working on Colab, a quick way to download images using image urls directly to your Colab working directory, you can use `wget` to do so. For example try to run `!wget [https://mymodernmet.com/wp/wp-content/uploads/2018/01/Tatsuto-Shibata-tokyo-photography-13.jpg](https://mymodernmet.com/wp/wp-content/uploads/2018/01/Tatsuto-Shibata-tokyo-photography-13.jpg)` in a cell and look refresh your current Colab directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM5sbYaC4bVG"
      },
      "source": [
        "!wget https://mymodernmet.com/wp/wp-content/uploads/2018/01/Tatsuto-Shibata-tokyo-photography-13.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqUfk-0p4l4-"
      },
      "source": [
        "objDet = Detector(model=\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\", device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaS0XsWT5Tbu"
      },
      "source": [
        "img = Image.open(\"Tatsuto-Shibata-tokyo-photography-13.jpg\")\r\n",
        "img_cv = utils.pil_to_cv2(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45vBfrwg5xbs"
      },
      "source": [
        "output = objDet.predict(img_cv)\r\n",
        "out_img = objDet.visualize(img_cv, output, figsize=(18,18))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG2KiCJd4_NF"
      },
      "source": [
        "objSegDet = Detector(model=\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7gWJkan554A"
      },
      "source": [
        "output = objSegDet.predict(img_cv)\r\n",
        "out_img = objSegDet.visualize(img_cv, output, figsize=(18,18))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EBAsksg7NP4"
      },
      "source": [
        "2. Now, try to modify the `Detector` from above and perform pose-estimation on the same image instead. An example of pose-estimation is as shown in the image below. Also, there is more information on what pose-estimation is in a link given in the Further Readings article for this chapter.\r\n",
        "\r\n",
        "***Hint***: *Check out Lesson 2 of this chapter if you have not!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLBKFOrw6LMj"
      },
      "source": [
        "poseDet = Detector(model=\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0lQ-Fm8VGtq"
      },
      "source": [
        "!mkdir processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8rlpNB36wwF"
      },
      "source": [
        "output = poseDet.predict(img_cv)\r\n",
        "out_img = poseDet.visualize(img_cv, output, figsize=(18,18), noplot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUP0gQmFVUvj"
      },
      "source": [
        "poseDet.save_image(out_img, 'processed', file_name='japanPosers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScnVIPCF7SMT"
      },
      "source": [
        "3. Now the harder challenge yet, try to perform the same task as `Q2` but this time do so with a video (e.g. image sequence)! Use the video located at [`https://drive.google.com/file/d/1Wv_-IUQtNFEs5xv3wFEerwymQXvZ_gsR/view?usp=sharing`](https://drive.google.com/file/d/1Wv_-IUQtNFEs5xv3wFEerwymQXvZ_gsR/view?usp=sharing), a beautiful little 20 second sequence that I have found online. Your task would be to apply the model (e.g perform model inference) on all the frames in the sequence.\r\n",
        "\r\n",
        "[Input Description]\r\n",
        "A video sequence (from above) in the form of jpeg images.\r\n",
        "\r\n",
        "[Output Description] \r\n",
        "The same jpeg images, but this time, with all the boxes and labels from the model drawn on them. You do not need to save the these images (e.g. output from the model) out as actual jpegs in your local directory. Just visualizing the images in your Notebook cell is sufficient.\r\n",
        "\r\n",
        "***Hint***: *First try to convert the video into a sequence of jpeg images. You can convert video files such as mp4 into jpeg images very simply through existing online tools - try googling for it! Alternatively, you can always use other 3rd party tools to do so, such as `ffmpeg` if you're familiar with it.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eONzoP6W61pH"
      },
      "source": [
        "%cp drive/MyDrive/A_zz_random_stuffs/Books/1stdayhackReadings/nyc_native_3_4.mp4 nyc_native_3_4.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnDirzGTieII"
      },
      "source": [
        "!rm -rf nyc_vid_frames\r\n",
        "!rm -rf processed_nyc_vid_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-PfFtKy8srL"
      },
      "source": [
        "!mkdir nyc_vid_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEmxWZVmTWS6"
      },
      "source": [
        "!mkdir processed_nyc_vid_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpmbpE6KZFwp"
      },
      "source": [
        "finalFPS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90EjVEmiAiNV"
      },
      "source": [
        "objdet = Detector(model=\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\", device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVACtQ18_xja"
      },
      "source": [
        "import cv2\r\n",
        "vidcap = cv2.VideoCapture('nyc_native_3_4.mp4')\r\n",
        "def getFrame(sec):\r\n",
        "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\r\n",
        "    hasFrames,image = vidcap.read()\r\n",
        "    if hasFrames:\r\n",
        "        cv2.imwrite(\"nyc_vid_frames/frame\"+str(count)+\".jpg\", image)     # save frame as JPG file\r\n",
        "    return hasFrames\r\n",
        "sec = 0\r\n",
        "frameRate = 1/finalFPS #capture frame every <1/finalFPS> second\r\n",
        "count=1\r\n",
        "success = getFrame(sec)\r\n",
        "while success:\r\n",
        "    count = count + 1\r\n",
        "    sec = sec + frameRate\r\n",
        "    sec = round(sec, 2)\r\n",
        "    success = getFrame(sec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIAVR68ZSfoY"
      },
      "source": [
        "import os\r\n",
        "frames = os.listdir('nyc_vid_frames')\r\n",
        "\r\n",
        "for i in range(1, len(frames)+1):\r\n",
        "  frame = Image.open('nyc_vid_frames/frame'+ str(i) +'.jpg')\r\n",
        "  frame_cv = utils.pil_to_cv2(frame)\r\n",
        "\r\n",
        "  frame_output = objdet.predict(frame_cv)\r\n",
        "  frame_output_img = objdet.visualize(frame_cv, frame_output, figsize=(18,18), noplot=True)\r\n",
        "\r\n",
        "  filename = 'p_frame'+str(i)\r\n",
        "\r\n",
        "  objdet.save_image(frame_output_img, 'processed_nyc_vid_frames', file_name=filename)\r\n",
        "\r\n",
        "  # so we know until where it going\r\n",
        "  print('processed '+str(i)+'th frame')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpLJu73LBKQG"
      },
      "source": [
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "from os.path import isfile, join\r\n",
        "\r\n",
        "pathIn= './processed_nyc_vid_frames/'\r\n",
        "pathOut = './processed/video.avi'\r\n",
        "fps = finalFPS\r\n",
        "frame_array = []\r\n",
        "files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\r\n",
        "\r\n",
        "#for sorting the file names properly\r\n",
        "files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\r\n",
        "print(files)\r\n",
        "\r\n",
        "for i in range(len(files)):\r\n",
        "    filename=pathIn + files[i]\r\n",
        "    #reading each files\r\n",
        "    img = cv2.imread(filename)\r\n",
        "    height, width, layers = img.shape\r\n",
        "    size = (width,height)\r\n",
        "    \r\n",
        "    #inserting the frames into an image array\r\n",
        "    frame_array.append(img)\r\n",
        "out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\r\n",
        "for i in range(len(frame_array)):\r\n",
        "    # writing to a image array\r\n",
        "    out.write(frame_array[i])\r\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkb3rmRK7VLa"
      },
      "source": [
        "4. [Optional, no answer provided] You could go the full mile and try to recreate what I did with the [demo](https://github.com/1stDayHack/FDK/raw/master/misc/readme/nyc_final_1.gif) on the 1stDayKit repo main page by saving all the resulting output images, and then converting said images back into a mp4 video for bonus awesome points ðŸŒŸ. Try to create a video whereby the first half of it (e.g. half the frames) is raw (e.g. with no model output applied) and the second half consisting of inferred frames (e.g. with model output applied). In addition, use the `PIL` to put a text in a corner of your video frame to indicate whether it is displaying the `raw` images or the `output` images. \r\n",
        "\r\n",
        "***Hint***: *You can refer to this* [`https://github.com/1stDayHack/FDK/blob/master/demo_1.ipynb`](https://github.com/1stDayHack/FDK/blob/master/demo_1.ipynb) for more hints!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6soEkqt97xNl"
      },
      "source": [
        "# this code is left as a trivial practice for the code-examiner"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}